الامر يقوم بتنظيف الدومينات الرئيسية فى الرابط المكررة اكثر من 50 مرة ويحظفهم دون اضافة /

awk -F'/' '{sub(":[0-9]+","",$3); domains[$3]++} {lines[NR]=$0} END { for (i in domains) { if (domains[i] <= 50) { for (j=1; j<=NR; j++) { if (lines[j] ~ i) print lines[j] } } } }' final_results.txt | sed 's| | /|g' > final_output.txt

يزيل الفراغ ويضيف /
sed 's| /|/|g' final_output.txt > final_output_fixed.txt
====================================================================
#!/bin/bash

# Script: code httpx Targets

    # Script: code httpx Targets
    echo "Running HTTPX..."

    cat final_results.txt | docker run -i projectdiscovery/httpx -status-code -nc -v | awk '/200/ {print $1}' | sed "s/$/'/g" > valid_urls1.txt
    cat valid_urls1.txt | docker run -i projectdiscovery/httpx -status-code -nc -v >> valid_urls2.txt

    # Extract URLs with "[Index of /]" in the title and store them in fileVOIP.txt
    echo "Filtering the results..."
    awk '/ \[404]/ && $1 ~ /^http[s]?:\/\/.*\.(cfg|xml|txt|log|conf|htm|ini|cnf|dat|boot|template|xsl|pcfg|prd|tfw|csv|gz|zip|tar|secret|passwd|pwd|key|backup|rar|bak|config|cvs|env|ftp-access|ftppass|logfile|login|login_conf|members|memdump|sql)\047$/ {print $1}' valid_urls2.txt >> link.txt
    sed "s/'$//" link.txt > HTTPXfileVOIP.txt

echo "Filtering completed! Filtered results are saved in the file"


awk -F'/' '{sub(":[0-9]+","",$3); domains[$3]++} {lines[NR]=$0} END { for (i in domains) { if (domains[i] <= 50) { for (j=1; j<=NR; j++) { if (lines[j] ~ i) print lines[j] } } } }' HTTPXfileVOIP.txt | sed 's| | /|g' > final_output.txt

sed 's| /|/|g' final_output.txt > fileVOIP.txt



rm valid_urls1.txt
rm valid_urls2.txt
rm filefile.txt
rm link.txt
rm final_output.txt
rm final_output.txt

echo "downloaded is done!"
echo "Thank you , Mohammad Elnwajha!"



===============================================



للبحث فى ملف دومينات فرعية عن كلمات
 
grep -E ".*[A-Za-z0-9]{0,4}(aastra|account|authentication|avaya|billing|call|carrier|cfg|cisco|client|cloud|conference|config|configuration|device|dialing|dialplan|fax|forward|gateway|grandstream|gxp|hosted|inbound|ivr|linksys|log|mobile|outbound|panasonic|pbx|phone|polycom|prov|provision|provisioning|proxy|reg|registration|secure|server|session|sip|snom|switch|tele|telecom|trunk|voice|voip|webphone|yealink|3cx|mitel|sangoma|digium|huawei|gigaset|asterisk|fanvil|opensips|siemens|freepbx|elastix|openvox|systems|patton|atcom|ipoffice|yeastar|freeswitch|omnipcx|nortel|ericsson|broadband|auto|elastic|firmware|fw|tftp)[A-Za-z0-9]{0,4}.*" results555.txt >> mohammad020.txt


حدف المكرر
awk '!seen[$0]++' mohammad020.txt > mohammad020_no_duplicates.txt

حدف الدومينات الفرعية الموجود فيها ايبيهات مع بعض
grep -vE '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+(\.[a-zA-Z0-9]+)+' mohammad020_no_duplicates.txt > newfile.txt


يستخدم هذا الامر عند مقارنة ملفين وحدف الموجود الاسطر الموجود فى ملف واحد من الملف اتنين

dos2unix patterns.txt

grep -vFf domain.txt fulldomain.txt > filtered_domains.txt
هذا  domain.txt هو الملف الذي فيه دومينات اساسية نريد مقارنتها بملف الدومينات الكاملة وحدفها من هنا


ملف الدومينات الكاملة والدومينات الفرعية  fulldomain.txt

ملف الحفظ  filtered_domains.txt
=======

أمر استبدال 

sed 's/\[.\]/./g' input.txt > output.txt

أمثلة
08auto[.]cn
0958699518[.]com
131cai[.]com
15550019997[.]com
163[.]com

النتيجة
08auto.cn
0958699518.com
131cai.com
15550019997.com
163.com
==========


sed  's/.*\.\([^.]\+\.[^.]\+\)$/\1/' moh.txt > 415263.txt

حدف الدومينات الرئيسية المكررة مثال  x.x  يحتفظ فى واحد فقط ويحدف الباقي الذي يكون بنفس اسم الدومين بالظبط

awk -F '.' '!seen[$1]++' resultsname.txt >> newresult.txt

awk -F '.' '!seen[$1]++' 1 >> 2
يعمل على 3 اجزء ويحدفهم ويبقي واحد منهم
awk -F '.' '!seen[$2"."$3]++' 3 >> 4
=========================

للبحث في الدومينات الفرعيىة عن 

للارقام فقط  وبدون ارقام 

grep -E "avaya[0-9]*\..*" file.txt >> results-24.txt



============================================================
للبحث عن حروف بعدد من 1 الى 6 بعد كلمة البحث

grep -E "avaya[A-Za-z]{1,6}\b" file.txt >> results-31.txt



امر لحدف الدومينات الاساسية المكررة مع الدومينات الفرعية المرتبطة بهم

awk -F '.' '!seen[$(NF-1)"."$NF]++' subdomainsMOHAMMAD.txt > output50.txt

يقوم بحدف اسطر الدومينات المكررة اكثر من 5 مرات بهيك بحتفظ بكثير نتائج دون مشاكل
awk -F '.' '{ domain = $(NF-1)"."$NF; count[domain]++ } END { for (d in count) if (count[d] <= 5) print d }' subdomainsMOHAMMAD.txt > output50.txt



ipswjw0073atl2.public.registeredsite.com

معرفة الدومينات الاساسية المكررة التى تم حدفها

awk -F '.' '++seen[$(NF-1)"."$NF] > 1 { print $(NF-1)"."$NF }' subdomains.txt > duplicate_domains.txt

